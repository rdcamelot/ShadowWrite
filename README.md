核心需求：**需要一个不仅能保存聊天，还能“实时、静默追加”到本地文档（如 Markdown）的方案，以便在不打断对话的前提下，随时在文档上对小说或推演内容进行润色和修改。同时，还要避免长对话导致的上下文截断问题。**

为了满足这个需求，下面是这三个技术方向的原理、优缺点以及实践建议：

---

## 1. 传统浏览器 DOM 插件（“事后快照”流）

这类工具适合**轻度提取**，你可以先下载体验一下，看看它们的源码，了解浏览器是如何与网页内容交互的。

* **工作原理：** 插件通过执行 JavaScript 脚本，直接读取你当前网页的 HTML DOM 树（也就是网页的骨架）。它会寻找特定的 CSS 类名（比如 `<div data-message-author-role="assistant">`），把里面的文字抠出来，转换成 Markdown 格式，然后打包成一个文件让你下载。
* **代表工具及源码参考：**
* **ChatGPT Exporter:** 在 GitHub 上开源，你可以去搜索它的源码，重点看它是如何解析 DOM 节点的（通常在 `content.js` 文件中）。
* **MarkDownload:** 一个通用的网页转 Markdown 工具，源码同样在 GitHub 可查。你可以学习它内部使用的 HTML 转 Markdown 的核心逻辑（如 `Turndown.js` 库）。


* **为何无法满足你的终极需求：**
* **非实时：** 只能手动点击触发，每次都会生成一个全新的文件，无法与你正在编辑的文档双向同步。
* **虚拟滚动（懒加载）陷阱：** 如果你的小说对话拉得很长，网页为了节省内存会把上面的对话从 DOM 里删掉。插件抓不到不在屏幕上的内容，导致你的长篇大论直接丢失。



---

## 2. 官方 API + 纯本地脚本（“硬核全场景”流）

这是最稳妥、最符合你“沉浸式写作与润色”需求的方案。跳过现成的网页界面，自己造一个纯本地的对话环境。

* **工作原理：**
你用 Python 或 Node.js 写一个脚本，直接通过 API（如 Gemini API、OpenAI API）发送你的提示词。脚本收到 AI 的回复后，做两件事：
1. 在屏幕（终端）上打印出来给你看。
2. 调用系统的文件操作命令（如 Node.js 的 `fs.appendFileSync`），把这段回复静默追加到你指定的 `novel_draft.md` 文件末尾。


* **完美契合你的工作流：**
你可以直接在 VS Code 里操作——左侧开一个内置终端（Terminal）专门用来和 AI 对话、输入剧情指令；右侧打开那个 `novel_draft.md` 文件。随着你在左边聊天，右边的文档会自动往下写，你可以随时在右侧修改设定、润色文笔，两边互不干扰。
* **优缺点：**
* **优点：** 100% 实时镜像，绝对不会因为网页崩溃或懒加载丢失任何历史记录；本地处理，隐私最安全。
* **缺点：** 失去了官方网页端花哨的 UI 界面；调用 API 可能会消耗额度（Token 计费）。



---

## 3. 油猴脚本 + 本地 Node.js 监听服务（“极客缝合怪”流）

如果你既想白嫖网页端免费且直观的对话界面，又想实现“实时同步到本地文档”的体验，这就是最终的折中方案。

* **工作原理：**
这是一个“里应外合”的架构，分为两部分：
1. **前端（监听者）：** 在浏览器里安装 Tampermonkey（油猴插件），写一段简单的脚本注入到聊天网页中。这段脚本利用 `MutationObserver` API，死死盯住网页的对话框区域。只要 AI 吐出了一段新话，脚本就立刻把这段话抓取下来。
2. **后端（接收者）：** 在你的电脑上跑一个非常极简的 Node.js 本地服务器（比如运行在 `localhost:3000`）。
油猴脚本抓到新内容后，通过 HTTP POST 请求，悄悄发给你的本地 Node.js 服务。Node.js 收到后，立刻将其追加（Append）到你的本地 Markdown 文件里。


* **优缺点：**
* **优点：** 巧妙绕过了浏览器的本地文件读写限制；即使网页开启了虚拟滚动，因为你是“增量实时抓取”，旧记录被清理了也无所谓，反正已经存到本地了。
* **缺点：** 维护成本稍高。如果 AI 平台某天更新了网页的前端代码（比如改了 CSS 类名），你的油猴脚本可能就会失效，需要重新检查网页元素并更新脚本。


### 现有产品的分析
Chat Memo 的核心原理：增量监听 + 浏览器本地存储

传统的插件是“拍快照”，而 Chat Memo 的核心逻辑更像是一个**“浏览器内置的录音机”**。

* **实时监听（MutationObserver）：** 它也是在网页里注入了脚本，利用 `MutationObserver` 实时盯着聊天框。AI 只要蹦出一个新字，它就立刻抓取下来。
* **沙盒内存储（IndexedDB / chrome.storage）：** **这是它和我们“Node.js 方案”最大的区别。** 抓到新文字后，它并没有穿透浏览器写到你电脑的 D 盘或桌面上，而是把数据塞进了**浏览器内部的本地数据库**（比如 IndexedDB）。
* **按需导出：** 当你想看的时候，你可以打开插件的独立面板，里面有你所有的聊天记录（从浏览器数据库里读出来的）。此时你可以点击“导出”，它会调用浏览器的下载功能，给你生成一个 `.md` 或 `.zip` 文件。

**好消息是，它完美解决了“长对话截断（懒加载）”的问题。**
因为它是“边聊边记”，所以无论网页怎么为了省内存而清理上面的对话节点，Chat Memo 早已把数据存进它自己的内部数据库里了。你的长篇小说推演不会丢。

**坏消息是，它依然打破不了“实时沉浸式双屏写作”的壁垒。**
由于浏览器的安全沙盒机制，Chat Memo 依然**无法直接操作你电脑硬盘上的文件系统**。
这意味着：

1. 你**不能**在 VS Code 里打开一个 Markdown 文件，然后一边在网页里和 AI 聊天，一边看着这个文件自动往下写。
2. 你的数据被“困”在了浏览器的内部存储里。如果你想在外部编辑器里润色，你依然需要经历：**打开插件 -> 点击导出 -> 下载新文件 -> 在本地打开修改** 这样的割裂流程。如果你在本地文件修改了前半部分，接下来导出的新文件依然无法和你本地的修改自动合并。

| 功能维度 | 传统 DOM 插件 (如 ChatGPT Exporter) | Chat Memo | 你的终极理想型 (网页监听 + Node.js 服务) |
| --- | --- | --- | --- |
| **工作模式** | 静态快照（事后算账） | 动态录音机（边聊边记） | 实时管道（边聊边穿透写入硬盘） |
| **是否防长对话丢失** | ❌ 否（网页滑不上去的就丢了） | ✅ 是（记录在浏览器内部数据库） | ✅ 是（直接写进本地 Markdown） |
| **存储位置** | 无，直接生成下载文件 | 浏览器的内部存储沙盒 | **你电脑上的任意文件夹** |
| **是否支持 VS Code 实时镜像润色** | ❌ 否 | ❌ 否（需要手动导出） | ✅ **是**（文件就在本地，随时可改，随时追加） |
| **隐私安全性** | 高（纯本地提取） | 高（不上传第三方服务器） | 极高（纯本地网络循环） |

Chat Memo 是一个非常优秀的折中产品。它把浏览器插件能做到的极限（增量保存 + 内部数据库）做到了最好。如果你觉得“边聊边在另一个窗口实时改代码/改小说”并不是绝对的刚需，只是想确保**“聊得再长也不丢数据，最后能完整导出”**，那么直接安装使用 Chat Memo 是性价比最高的选择，你完全不需要自己写代码了。

但如果你追求的是那种极致的极客体验——**左半屏是网页聊天，右半屏是 VS Code 里的 `.md` 自动打字，并且你随时可以停下来在右边直接改大纲和设定**——那么浏览器沙盒这堵墙你必须得翻过去，也就是必须得跑一个本地的 Node.js 接收服务。

---

## 4. 当前项目进展（2026-02-20）

### 已完成

1. 已克隆并建立参考代码库：
   - `external/chatgpt-exporter`
   - `external/markdownload`
   - `external/turndown`
2. 已实现“方案 2”的本地可运行 MVP（无第三方依赖）：
   - `shadowwrite_cli.py`
   - `.env.example`
   - `LOCAL_API_WORKFLOW.md`
3. 当前 CLI 能力：
   - 终端连续对话
   - OpenAI-compatible 与 Gemini 的流式响应（边显示边写入）
   - 用户输入默认折叠保存（`<details>`），AI 回复作为主体正文
   - 支持 Typora 兼容回退（`SHADOWWRITE_USER_INPUT_MODE=blockquote`）
   - 时间默认可隐藏，仍保留可索引元数据
   - 每轮对话自动追加到本地 Markdown（默认 `novel_draft.md`）
   - 可选同步输出聊天风格 HTML 视图（默认 `novel_draft.chat.html`）
   - HTML 视图支持助手 Markdown 渲染（CDN 可用时）
   - 支持手动结构化命令：`/section`、`/note`
   - 支持快照命令：`/snapshot`
   - 适配 `openai_compat` 与 `gemini` 两种接口层

### 方案 2 后续功能优先级（更新）

#### P0（已完成）

- **流式输出 + 流式写入 `.md`**
  - 必要性：高。它直接决定“沉浸感”，让右侧文档像“实时打字”。
  - 对目标贴合度：显著提升。

#### P1（建议下一步做）

- **自动分章节/标签写入（剧情/设定/灵感）**
  - 必要性：中。不是 MVP 必需，但对长期写作管理很有价值。
  - 对目标贴合度：提升“可维护性”，不影响核心“实时镜像”能力。

#### P2（可选增强）

- 自动滚动锚点、断点恢复、按日归档、会话导出索引。

### 当前结论

先走“方案 2”是正确路线。它已经具备跨模型能力（通过接口适配层），下一阶段优先补齐流式体验，再做结构化写作分区。

### 路线 3 设计沉淀

- 详见：`ROUTE3_BASELINE.md`
- 内容包括：目标、现状、可借鉴点、路线 3 的 M0/M1 实现建议、风险与优先级。
